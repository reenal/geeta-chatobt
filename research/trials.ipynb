{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URI = 'mongodb+srv://parth:parth123@cluster0.1ngdui1.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_info(email):\n",
    "    document = {\n",
    "        'email':email\n",
    "    }\n",
    "    result = users_collection.find_one(document)\n",
    "    print(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_uri = os.getenv('MONGO_URI')\n",
    "client = MongoClient(mongo_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client[\"user_database\"]\n",
    "users_collection = db[\"users\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('665ae485b2f495c30ea37240'), 'email': 'prathameshshete609@gmail.com', 'password': b'$2b$12$lSVTHfXuRDrVSfkfE3Ds8ONfJAW1Zvu8hxT6ABPZArPp0UuYQFmqO', 'name': 'Prathamesh Dattatraya Shete', 'age': 'Male', 'gender': 0}\n",
      "{'_id': ObjectId('665ae485b2f495c30ea37240'),\n",
      " 'age': 'Male',\n",
      " 'email': 'prathameshshete609@gmail.com',\n",
      " 'gender': 0,\n",
      " 'name': 'Prathamesh Dattatraya Shete',\n",
      " 'password': b'$2b$12$lSVTHfXuRDrVSfkfE3Ds8ONfJAW1Zvu8hxT6ABPZArPp0UuYQFmqO'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "result = find_info('prathameshshete609@gmail.com')\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male\n"
     ]
    }
   ],
   "source": [
    "print(result['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Parth\\\\Desktop\\\\geeta-chatobt'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Parth\\Desktop\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents():\n",
    "    # logging.info('read data directory all pdfs')\n",
    "    loader = PyPDFDirectoryLoader(\"data/\")\n",
    "    docs = loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = read_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_chunks(text):\n",
    "    # logging.info('chunking process started for the extracted text')\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "    # logging.info('chucking process done successfully')\n",
    "    split_text = text_splitter.split_documents(text)\n",
    "    \n",
    "    return split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_text_chunks(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(text_chunks):\n",
    "    logging.info(\"start storing embeeding into vector db\")\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)\n",
    "    vector_store.save_local(\"faiss_index\")\n",
    "    logging.info(\"storing embeeding into vector db done successfully in local folder\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m db\u001b[38;5;241m=\u001b[39m get_vector_store(text)\n",
      "Cell \u001b[1;32mIn[87], line 4\u001b[0m, in \u001b[0;36mget_vector_store\u001b[1;34m(text_chunks)\u001b[0m\n\u001b[0;32m      2\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart storing embeeding into vector db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m GoogleGenerativeAIEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/embedding-001\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(text_chunks, embedding\u001b[38;5;241m=\u001b[39membeddings)\n\u001b[0;32m      5\u001b[0m vector_store\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring embeeding into vector db done successfully in local folder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Parth\\anaconda3\\Lib\\site-packages\\langchain_core\\vectorstores.py:550\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    549\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Parth\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:931\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    930\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m--> 931\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m    932\u001b[0m     texts,\n\u001b[0;32m    933\u001b[0m     embeddings,\n\u001b[0;32m    934\u001b[0m     embedding,\n\u001b[0;32m    935\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    936\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    938\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Parth\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:888\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    885\u001b[0m     index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;66;03m# Default to L2, currently other metric types not initialized.\u001b[39;00m\n\u001b[1;32m--> 888\u001b[0m     index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m    889\u001b[0m docstore \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocstore\u001b[39m\u001b[38;5;124m\"\u001b[39m, InMemoryDocstore())\n\u001b[0;32m    890\u001b[0m index_to_docstore_id \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_to_docstore_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "db= get_vector_store(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd c:\\Users\\Parth\\Desktop\\geeta-chatobt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.helper import *\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import random\n",
    "from src.registration import *\n",
    "from src.mongo import *\n",
    "from pages import *\n",
    "# from streamlit_option_menu import option_menu\n",
    "from src.logger import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who am i ?\n",
      "You are an eternal soul, a fragment of the divine, distinct from the temporary body. \n",
      "\n",
      "Shloka: नैनं छिन्दन्ति शस्त्राणि नैनं दहति पावकः । न चैनं क्लेदयन्त्यापो न शोषयति मारुतः ।। (2.23)\n",
      "\n",
      "Meaning: Weapons cannot cut the soul, fire cannot burn it, water cannot wet it, and wind cannot dry it.\n",
      "\n",
      "Example: Just as a drop of ocean water contains the essence of the entire ocean, you possess within you the essence of the divine. You are not the body, but the eternal soul residing within. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-15 18:55:04.999 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\Parth\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "question = 'who am i ?'\n",
    "\n",
    "response =  user_input(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an eternal soul, a fragment of the divine, distinct from the temporary body. \n",
      "\n",
      "Shloka: नैनं छिन्दन्ति शस्त्राणि नैनं दहति पावकः । न चैनं क्लेदयन्त्यापो न शोषयति मारुतः ।। (2.23)\n",
      "\n",
      "Meaning: Weapons cannot cut the soul, fire cannot burn it, water cannot wet it, and wind cannot dry it.\n",
      "\n",
      "Example: Just as a drop of ocean water contains the essence of the entire ocean, you possess within you the essence of the divine. You are not the body, but the eternal soul residing within. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_text():\n",
    "    logging.info('read the pdf dat from the data folder')\n",
    "    text = \"\"\n",
    "    pdf_reader = PdfReader(path)\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    logging.info('extract text from pdf successfully')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(text_chunks):\n",
    "    logging.info(\"start storing embeeding into vector db\")\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "    vector_store.save_local(\"faiss_index_2\")\n",
    "    logging.info(\"storing embeeding into vector db done successfully in local folder\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Parth\\\\Desktop\\\\geeta-chatobt'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Parth\\Desktop\\projects\\geeta-chatobt\\data\\The-Way-of-the-Monk.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(path)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding():\n",
    "    logging.info('embedding process started')\n",
    "    text = get_file_text()\n",
    "    text_chunks = get_text_chunks(text)\n",
    "    db = get_vector_store(text_chunks=text_chunks)\n",
    "    logging.info('embedding process done successfully')\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = create_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_answer(user_question):\n",
    "    # Create a prompt template for predicting the next question\n",
    "    prompt_template = \"\"\"\n",
    "    Based on the response extact example only from the response and suggest me the best example to support the answer {user_question} as per the context {context}\n",
    "    \"\"\"\n",
    "    new_db = FAISS.load_local(\"faiss_index_2\", embeddings, allow_dangerous_deserialization=True)\n",
    "    logging.info('load vector database for successfully')\n",
    "    # Perform a similarity search and retrieve relevant documents as context\n",
    "    docs = new_db.similarity_search(user_question)\n",
    "    logging.info('similarity search done successfully')\n",
    "    # Get the conversational chain\n",
    "    chain = get_conversational_chain()\n",
    "    logging.info('conversational chain initialize')\n",
    "    \n",
    "    # Define the language model\n",
    "    model = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro-latest\", temperature=0.3)\n",
    "    \n",
    "    # Create the prompt and LLM chain\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=['context',\"user_question\"])\n",
    "    \n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    \n",
    "    # Use the chain to predict the next question\n",
    "    next_question = chain.invoke({\"context\": docs, \"user_question\": user_question})\n",
    "    \n",
    "    return next_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'who is god'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is god\n",
      "God is the basis of the imperishable, the source of everything in this universe, the inexhaustible, the protector of eternal righteousness.\n",
      "\n",
      "Shloka:  त्वमक्षरं परमं वेदितव्यं  \n",
      "त्वमस्य विश्वस्य परं निधानम् |\n",
      "त्वमव्ययः शाश्वतधर्मगोप्ता\n",
      " सनातनस्त्वं पुरुषो मतो मे || 18||\n",
      "\n",
      "Meaning: You are the imperishable, the Supreme Being, the ultimate reality to be known. You are the refuge of this universe, the inexhaustible, the protector of eternal dharma, the eternal one, the Supreme Divine Personality – this is my understanding.\n",
      "\n",
      "Explanation: This shloka describes God as the eternal, unchanging reality behind the ever-changing universe. He is the source of everything and the ultimate goal. He is the protector of righteousness and the embodiment of truth.\n",
      "\n",
      "Example: Imagine a spider creating a web. The web comes from the spider, exists because of the spider, and can be withdrawn into the spider. Similarly, the entire universe emanates from God, is sustained by God, and ultimately rests in God. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = user_input(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rewrite_answer(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God is the basis of the imperishable, the source of everything in this universe, the inexhaustible, the protector of eternal righteousness.\n",
      "\n",
      "Shloka:  त्वमक्षरं परमं वेदितव्यं  \n",
      "त्वमस्य विश्वस्य परं निधानम् |\n",
      "त्वमव्ययः शाश्वतधर्मगोप्ता\n",
      " सनातनस्त्वं पुरुषो मतो मे || 18||\n",
      "\n",
      "Meaning: You are the imperishable, the Supreme Being, the ultimate reality to be known. You are the refuge of this universe, the inexhaustible, the protector of eternal dharma, the eternal one, the Supreme Divine Personality – this is my understanding.\n",
      "\n",
      "Explanation: This shloka describes God as the eternal, unchanging reality behind the ever-changing universe. He is the source of everything and the ultimate goal. He is the protector of righteousness and the embodiment of truth.\n",
      "\n",
      "Example: Imagine a spider creating a web. The web comes from the spider, exists because of the spider, and can be withdrawn into the spider. Similarly, the entire universe emanates from God, is sustained by God, and ultimately rests in God. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text does not contain an example that directly illustrates the concept of God as the basis of the imperishable, the source of everything, and the protector of eternal righteousness. The example of the spider and the web is a common analogy for the relationship between God and the universe, but it is not present in this text. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img_generator(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_generator(img_prompt):\n",
    "    logging.info(\"Image generator function started\")\n",
    "    \n",
    "    API_URL = \"https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "    headers = {\"Authorization\": 'Bearer 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'}\n",
    "\n",
    "    logging.info(\"Sending request to image generation API\")\n",
    "    image_bytes = query_image_generation(API_URL, headers, {\"inputs\": img_prompt})\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = img_generator(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'JpegImageFile' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image               \u001b[38;5;66;03m# to load images\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display \u001b[38;5;66;03m# to display images\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m pil_im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image)\n\u001b[0;32m      5\u001b[0m display(pil_im)\n",
      "File \u001b[1;32mc:\\Users\\Parth\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3286\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3283\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m   3284\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 3286\u001b[0m prefix \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m   3288\u001b[0m preinit()\n\u001b[0;32m   3290\u001b[0m accept_warnings \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'JpegImageFile' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "from PIL import Image               # to load images\n",
    "from IPython.display import display # to display images\n",
    "\n",
    "pil_im = Image.open(image)\n",
    "display(pil_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
